{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 64, 64, 3)\n",
      "(1080,)\n",
      "test_x shape is (120, 64, 64, 3) test_y shape is (120,)\n",
      "train_x shape is (12288, 1080) test_x shape is (12288, 120)\n"
     ]
    }
   ],
   "source": [
    "train=h5py.File('train_signs.h5','r')\n",
    "train_orig_x=np.array(train['train_set_x'])\n",
    "train_orig_y=np.array(train['train_set_y'])\n",
    "print(train_orig_x.shape)\n",
    "print(train_orig_y.shape)\n",
    "test=h5py.File('test_signs.h5','r')\n",
    "list(test.keys())\n",
    "test_orig_x=np.array(test['test_set_x'])\n",
    "test_orig_y=np.array(test['test_set_y'])\n",
    "print('test_x shape is {} test_y shape is {}'.format(test_orig_x.shape,test_orig_y.shape))\n",
    "#reshaping the data in the form of (nx,m) where m in number of examples\n",
    "train_x_flat=train_orig_x.reshape(train_orig_x.shape[0],-1).T\n",
    "test_x_flat=test_orig_x.reshape(test_orig_x.shape[0],-1).T\n",
    "train_y=train_orig_y\n",
    "test_y=test_orig_y\n",
    "#reshaping the data \n",
    "train_x=train_x_flat/255.\n",
    "test_x=test_x_flat/255.\n",
    "print('train_x shape is {} test_x shape is {}'.format(train_x.shape,test_x.shape)) \n",
    "#now choosing the train and test set for only 2 classes 2 class is taken as 1 and rest are 1,3,4 is taken as 0\n",
    "train_cl_df=pd.DataFrame(train_orig_y,columns=['Class_labels'])\n",
    "li_2_train=train_cl_df[train_cl_df.Class_labels==2].index.tolist()\n",
    "train_orig_x_2=train_orig_x[li_2_train]\n",
    "li_1_3_4_train=train_cl_df[train_cl_df.Class_labels.isin([1,3,4])].index.tolist()\n",
    "train_orig_x_1_3_4=train_orig_x[li_1_3_4_train]\n",
    "train_orig_2_1_3_4=np.concatenate((train_orig_x_2,train_orig_x_1_3_4),axis=0)\n",
    "#doing random shuffling before flattening\n",
    "np.random.seed(3)\n",
    "shuffle=np.random.randint(0,720,size=720)\n",
    "train_orig_2_1_3_4=train_orig_2_1_3_4[shuffle]\n",
    "train_orig_x_2=train_orig_2_1_3_4.reshape(train_orig_2_1_3_4.shape[0],-1).T\n",
    "train_orig_x_2#this is the train data with 2 classes \n",
    "train_y_2_1_3_4=np.concatenate((np.ones(180),np.zeros(540)))\n",
    "train_orig_y_2=train_y_2_1_3_4[shuffle]\n",
    "train_x_2=train_orig_x_2/255.0\n",
    "#similarly preparing the test data \n",
    "test_cl_df=pd.DataFrame(test_orig_y,columns=['class_labels'])\n",
    "li_test_2=test_cl_df[test_cl_df.class_labels==2].index.tolist()\n",
    "li_test_1_3_4=test_cl_df[test_cl_df.class_labels.isin([1,3,4])].index.tolist()\n",
    "test_orig_x_2=np.concatenate((test_orig_x[li_test_2],test_orig_x[li_test_1_3_4]),axis=0)\n",
    "test_orig_y_2=np.concatenate((np.ones(20),np.zeros(60)),axis=0)\n",
    "np.random.seed(3)\n",
    "test_shuffle=np.random.randint(0,80,80)\n",
    "test_shuffle\n",
    "test_orig_x_2=test_orig_x_2[test_shuffle]\n",
    "test_orig_y_2=test_orig_y_2[test_shuffle]\n",
    "test_x_2=test_orig_x_2.reshape(test_orig_x_2.shape[0],-1).T\n",
    "test_x_2=test_x_2/255.0\n",
    "test_orig_y_2=test_orig_y_2.reshape(test_orig_y_2.shape[0],-1).T\n",
    "train_orig_y_2=train_orig_y_2.reshape(train_orig_y_2.shape[0],-1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if n on-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X,W1,b1,activation='relu')\n",
    "        A2, cache2 = linear_activation_forward(A1,W2,b2,activation='sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A2,Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2,cache2,activation='sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1,cache1,activation='relu')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A)+b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = (np.dot(dZ,A_prev.T))/m\n",
    "    db = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -(np.sum(np.multiply(Y,np.log(AL))+np.multiply((1-Y),np.log(1-AL))))/m\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current laylinear_activation_forwarder l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1=np.random.randn(n_h,n_x)*0.01\n",
    "    b1=np.zeros([n_h,1])\n",
    "    W2=np.random.randn(n_y,n_h)*0.01\n",
    "    b2=np.zeros([n_y,1])\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6909447889515705\n",
      "Cost after iteration 100: 0.5286279752548879\n",
      "Cost after iteration 200: 0.5209773934658863\n",
      "Cost after iteration 300: 0.5051888578662531\n",
      "Cost after iteration 400: 0.4740155171125038\n",
      "Cost after iteration 500: 0.4296491094084164\n",
      "Cost after iteration 600: 0.45930843233176516\n",
      "Cost after iteration 700: 0.43839787519714396\n",
      "Cost after iteration 800: 0.4283539429712511\n",
      "Cost after iteration 900: 0.4105835334300361\n",
      "Cost after iteration 1000: 0.401062759468696\n",
      "Cost after iteration 1100: 0.3899012272827242\n",
      "Cost after iteration 1200: 0.380381483811354\n",
      "Cost after iteration 1300: 0.3674810781089854\n",
      "Cost after iteration 1400: 0.35488979627738204\n",
      "Cost after iteration 1500: 0.3420639963825829\n",
      "Cost after iteration 1600: 0.34008072662268585\n",
      "Cost after iteration 1700: 0.3332844343050388\n",
      "Cost after iteration 1800: 0.3278470853785655\n",
      "Cost after iteration 1900: 0.3151601222803141\n",
      "Cost after iteration 2000: 0.32688552613354127\n",
      "Cost after iteration 2100: 0.29090893294910963\n",
      "Cost after iteration 2200: 0.27904365456617397\n",
      "Cost after iteration 2300: 0.2929760800340751\n",
      "Cost after iteration 2400: 0.27067399972697104\n",
      "Cost after iteration 2500: 0.25705794253468894\n",
      "Cost after iteration 2600: 0.27179909011426384\n",
      "Cost after iteration 2700: 0.24759288437928803\n",
      "Cost after iteration 2800: 0.25729730250070226\n",
      "Cost after iteration 2900: 0.26989645394550815\n",
      "Cost after iteration 3000: 0.23282816227530037\n",
      "Cost after iteration 3100: 0.21934095674727958\n",
      "Cost after iteration 3200: 0.21915498326289004\n",
      "Cost after iteration 3300: 0.20742762573573986\n",
      "Cost after iteration 3400: 0.20161186364428663\n",
      "Cost after iteration 3500: 0.20335523528977606\n",
      "Cost after iteration 3600: 0.19600242076473692\n",
      "Cost after iteration 3700: 0.27163512952623453\n",
      "Cost after iteration 3800: 0.17505856659051847\n",
      "Cost after iteration 3900: 0.16564556876701952\n",
      "Cost after iteration 4000: 0.2765201322728163\n",
      "Cost after iteration 4100: 0.16142634237515946\n",
      "Cost after iteration 4200: 0.1609507976927665\n",
      "Cost after iteration 4300: 0.15040721285870745\n",
      "Cost after iteration 4400: 0.2954133975698144\n",
      "Cost after iteration 4500: 0.14225389212773112\n",
      "Cost after iteration 4600: 0.1373603859147129\n",
      "Cost after iteration 4700: 0.13591807019495006\n",
      "Cost after iteration 4800: 0.1444579767952075\n",
      "Cost after iteration 4900: 0.12907073951414613\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_x=train_x_2.shape[0]\n",
    "n_h=7\n",
    "n_y=1\n",
    "parameters = two_layer_model(train_x_2, train_orig_y_2, layers_dims = (n_x, n_h, n_y), num_iterations = 5000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-ef6bdd800e2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_orig_y_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameters' is not defined"
     ]
    }
   ],
   "source": [
    "p=predict(test_x_2,test_orig_y_2,parameters=parameters)\n",
    "np.squeeze(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
